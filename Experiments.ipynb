{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PGU5mj_LId08"
      ],
      "authorship_tag": "ABX9TyPgwbRGeFskWT153lv/Sf0L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/SMIED/blob/main/Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPlZYxgBE-Z7",
        "outputId": "fbd24fbb-2922-4c91-f7be-803f7b096976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package framenet_v17 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Any, Dict, Tuple, Set\n",
        "import nltk\n",
        "nltk.download('framenet_v17')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import framenet as fn\n",
        "from nltk.corpus import wordnet as wn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get overlapping hypernym paths"
      ],
      "metadata": {
        "id": "PGU5mj_LId08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def overlapping_hypernym_paths(syn1, syn2) -> List[Any]:\n",
        "    lchs = syn1.lowest_common_hypernyms(syn2)\n",
        "    print(\"LCHs:\", [lch.name() for lch in lchs])\n",
        "    common_paths = []\n",
        "\n",
        "    for p1 in syn1.hypernym_paths():\n",
        "        for p2 in syn2.hypernym_paths():\n",
        "            if any(lch in p1 for lch in lchs) and any(lch in p2 for lch in lchs):\n",
        "              # truncate the paths until they've got one of the lchs\n",
        "              while p1 and p2 and p1[0] not in lchs:\n",
        "                  last_lch = p1[0]\n",
        "                  p1 = p1[1:]\n",
        "                  p2 = p2[1:]\n",
        "              # get the shared lch path\n",
        "              common_paths.append(p1[::-1] + p2[1:])\n",
        "\n",
        "    return common_paths"
      ],
      "metadata": {
        "id": "EYixUJx1HmAy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example:\n",
        "cat = wn.synset('cat.n.01')\n",
        "dog = wn.synset('dog.n.01')\n",
        "print()\n",
        "overlaps = overlapping_hypernym_paths(cat, dog)\n",
        "for path in overlaps:\n",
        "    print(\" → \".join(s.name() for s in path))\n",
        "print()\n",
        "overlaps = overlapping_hypernym_paths(dog, cat)\n",
        "for path in overlaps:\n",
        "    print(\" → \".join(s.name() for s in path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VScLUPMTHfGz",
        "outputId": "fb07339f-5c42-462d-988e-2ce8179a0120"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LCHs: ['carnivore.n.01']\n",
            "cat.n.01 → feline.n.01 → carnivore.n.01 → canine.n.02 → dog.n.01\n",
            "\n",
            "LCHs: ['carnivore.n.01']\n",
            "dog.n.01 → canine.n.02 → carnivore.n.01 → feline.n.01 → cat.n.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get frame info from predicate token"
      ],
      "metadata": {
        "id": "mFoGKsvjvcBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Token, Doc\n",
        "from spacy import displacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "hr31dbWvvf31"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get a dict of all dependency schemas matching a synset's frames/usage"
      ],
      "metadata": {
        "id": "VtA_LjJE0R-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Token\n",
        "\n",
        "# Register a custom extension for original POS if you want to store it\n",
        "if not Token.has_extension(\"orig_tag\"):\n",
        "    Token.set_extension(\"orig_tag\", default=None)\n",
        "\n",
        "@spacy.language.Language.component(\"custom_pos_modifier\")\n",
        "def custom_pos_modifier(doc):\n",
        "    mapping = {\"something\": \"NN\", \"someone\": \"NN\"}  # Use suitable tag string\n",
        "    for token in doc:\n",
        "        # Save original tag\n",
        "        token._.orig_tag = token.tag_\n",
        "\n",
        "        if token.text.lower() not in mapping and token.pos_ == \"NOUN\":\n",
        "            token.tag_ = \"VB\"  # Set fine-grained tag to verb\n",
        "        # Else, you could override mapping if needed\n",
        "\n",
        "    return doc\n",
        "\n",
        "arg_schema_nlp = spacy.load(\"en_core_web_sm\")\n",
        "arg_schema_nlp.add_pipe(\"custom_pos_modifier\", after=\"tagger\")\n",
        "print(arg_schema_nlp.pipe_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc9WPYWq_Dvx",
        "outputId": "0a38c544-d17e-412d-e800-f17d1a309ad9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tok2vec', 'tagger', 'custom_pos_modifier', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _flatten_tok_deps(tok: Token) -> Set[Token]:\n",
        "    deps = set()\n",
        "    for child in tok.children:\n",
        "        deps.add(child)\n",
        "        deps.update(_flatten_tok_deps(child))\n",
        "    return deps\n",
        "\n",
        "# TODO: retain original arg structure and add other syntactic/lexical info for better SRL\n",
        "def _flattened_fn_arg_schema(doc: Doc|Token) -> Dict[str, str]:\n",
        "      # used as fallback if FE parsing fails later on\n",
        "      schema_atom_synset_maps = {\n",
        "          \"something\": \"entity.n.01\",\n",
        "          \"someone\": \"causal_agent.n.01\",\n",
        "      }\n",
        "\n",
        "      # if it's a token, just flatten its children and call that set a doc\n",
        "      if isinstance(doc, Token):\n",
        "          doc: Set[Token] = _flatten_tok_deps(doc)\n",
        "\n",
        "      arg_schema = dict()\n",
        "      for tok in doc:\n",
        "          if tok.lower in schema_atom_synset_maps.keys():\n",
        "              arg_schema[tok.dep_] = schema_atom_synset_maps[tok.lower]\n",
        "          else:\n",
        "              arg_schema[tok.dep_] = tok.lemma_\n",
        "      return arg_schema"
      ],
      "metadata": {
        "id": "1p1LEnCu1arx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_dep_reqs(doc: Doc|Token) -> Dict[str, bool]:\n",
        "      arg_schema_dep_req_map = {\n",
        "          \"subject\": {\"nsubj\", \"nsubjpass\"},\n",
        "          \"object\": {\"dobj\", \"dative\"},\n",
        "          \"theme\": {\"iobj\"}\n",
        "      }\n",
        "\n",
        "      arg_schema_reqs = _flattened_fn_arg_schema(doc)\n",
        "\n",
        "      # restructure arg_reqs based to only evaluate core dependencies\n",
        "      #   TODO: add more relationships for finer-grained SRL\n",
        "      return {\n",
        "          k: len(v.intersection(set(arg_schema_reqs.keys()))) > 0\n",
        "          for k, v in arg_schema_dep_req_map.items()\n",
        "      }"
      ],
      "metadata": {
        "id": "zefKVmTqH3GV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_f_id_arg_struct_dict(syn: wn.synset) -> Dict[int, Dict[str, bool]]:\n",
        "      syn_frame_ids_strs: Dict[int, Doc] = dict()\n",
        "      for lemma in syn.lemmas():\n",
        "\n",
        "          # get all FrameNet frame IDs for this lemma\n",
        "          for i, f_id in enumerate(lemma.frame_ids()):\n",
        "\n",
        "              # get the argument structure for this frame as a string\n",
        "              f_str = lemma.frame_strings()[i]\n",
        "              # parse f_str into an argument structure vector\n",
        "              #   use tuple to make it hashable\n",
        "              f_arg_structure = f_str.split(' ')\n",
        "\n",
        "              # remove any extra arguments beyond subject, object, theme\n",
        "              if f_arg_structure[-1] != f_arg_structure[-1].lower():\n",
        "                  f_arg_structure = f_arg_structure[:-1]\n",
        "\n",
        "              # create a spacy doc for the argument structure template\n",
        "              doc = arg_schema_nlp(' '.join(f_arg_structure))\n",
        "              # # display dependency parse tree for debugging\n",
        "              # displacy.render(doc, style='dep')\n",
        "\n",
        "              syn_frame_ids_strs[f_id] = _get_dep_reqs(doc)\n",
        "\n",
        "      return syn_frame_ids_strs"
      ],
      "metadata": {
        "id": "4Xtwh7hLy6aM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_id_docs = _get_f_id_arg_struct_dict(wn.synset('spin.v.01'))"
      ],
      "metadata": {
        "id": "VkCbL9cbzh64"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Wordnet frames structures that match the structure of the dependency parse of the original predicate token"
      ],
      "metadata": {
        "id": "HCElGXuM0bCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_candidate_wn_frames(pred_tok: spacy.tokens.Token) -> Dict[str, Set[Tuple[bool, bool, bool]]]:\n",
        "      original_tok_dep_reqs = _get_dep_reqs(_get_dep_reqs)\n",
        "      vect_order = [\"subject\", \"object\", \"theme\"]\n",
        "\n",
        "      # get candidate WordNet synsets for the predicate (verbs)\n",
        "      pred_lemma = pred_tok.lemma_.lower()\n",
        "      # get a dict of synset names and synset objects for quick lookup\n",
        "      pred_synsets = wn.synsets(pred_lemma, pos=wn.VERB)\n",
        "      pred_synsets = {syn.name(): syn for syn in pred_synsets}\n",
        "      # create a dict of synset names and lists of their possible frames,\n",
        "      #   indexed by argument structure (num args of each type)\n",
        "      pred_frames: Dict[str, Set[Tuple[bool, bool, bool]]] = dict()\n",
        "\n",
        "      # get all FrameNet frames associated with this synset\n",
        "      for s_name, syn in pred_synsets.items():\n",
        "          for frame_id, arg_schema_reqs in _get_f_id_arg_struct_dict(syn).items():\n",
        "              # add wordnet's frame requirements as an entry in the synset's nested dict\n",
        "              arg_struct_tuple = tuple(arg_schema_reqs[k] for k in vect_order)\n",
        "              if s_name not in pred_frames:\n",
        "                  pred_frames[s_name] = set()\n",
        "              pred_frames[s_name].add(arg_struct_tuple)\n",
        "\n",
        "      return pred_frames"
      ],
      "metadata": {
        "id": "pU-sJtLzxPpo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "pprint.pprint(_get_candidate_wn_frames(nlp(\"strike\")[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "5Fh-ElkL5NSk",
        "outputId": "be954ab6-7343-44f0-a3d6-53b11e234678"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'function' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3155780475.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_candidate_wn_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"strike\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1727609871.py\u001b[0m in \u001b[0;36m_get_candidate_wn_frames\u001b[0;34m(pred_tok)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_candidate_wn_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tok\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m       \u001b[0moriginal_tok_dep_reqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_dep_reqs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_dep_reqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m       \u001b[0mvect_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"subject\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"object\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"theme\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;31m# get candidate WordNet synsets for the predicate (verbs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-464410890.py\u001b[0m in \u001b[0;36m_get_dep_reqs\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      6\u001b[0m       }\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0marg_schema_reqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_flattened_fn_arg_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;31m# restructure arg_reqs based to only evaluate core dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1140091052.py\u001b[0m in \u001b[0;36m_flattened_fn_arg_schema\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0marg_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mschema_atom_synset_maps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m               \u001b[0marg_schema\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdep_\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema_atom_synset_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "We267itBKn2Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}