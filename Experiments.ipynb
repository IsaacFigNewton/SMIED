{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PGU5mj_LId08"
      ],
      "authorship_tag": "ABX9TyMcK72YozI3QV7VqK3GE8u2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsaacFigNewton/SMIED/blob/main/Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPlZYxgBE-Z7",
        "outputId": "3b4cd97f-1258-4658-cc13-fe873fc94445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package framenet_v17 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Any, Dict, Tuple, Set\n",
        "import nltk\n",
        "nltk.download('framenet_v17')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import framenet as fn\n",
        "from nltk.corpus import wordnet as wn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get overlapping hypernym paths"
      ],
      "metadata": {
        "id": "PGU5mj_LId08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def overlapping_hypernym_paths(syn1, syn2) -> List[Any]:\n",
        "    lchs = syn1.lowest_common_hypernyms(syn2)\n",
        "    print(\"LCHs:\", [lch.name() for lch in lchs])\n",
        "    common_paths = []\n",
        "\n",
        "    for p1 in syn1.hypernym_paths():\n",
        "        for p2 in syn2.hypernym_paths():\n",
        "            if any(lch in p1 for lch in lchs) and any(lch in p2 for lch in lchs):\n",
        "              # truncate the paths until they've got one of the lchs\n",
        "              while p1 and p2 and p1[0] not in lchs:\n",
        "                  last_lch = p1[0]\n",
        "                  p1 = p1[1:]\n",
        "                  p2 = p2[1:]\n",
        "              # get the shared lch path\n",
        "              common_paths.append(p1[::-1] + p2[1:])\n",
        "\n",
        "    return common_paths"
      ],
      "metadata": {
        "id": "EYixUJx1HmAy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example:\n",
        "cat = wn.synset('cat.n.01')\n",
        "dog = wn.synset('dog.n.01')\n",
        "print()\n",
        "overlaps = overlapping_hypernym_paths(cat, dog)\n",
        "for path in overlaps:\n",
        "    print(\" → \".join(s.name() for s in path))\n",
        "print()\n",
        "overlaps = overlapping_hypernym_paths(dog, cat)\n",
        "for path in overlaps:\n",
        "    print(\" → \".join(s.name() for s in path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VScLUPMTHfGz",
        "outputId": "b7c06d33-ed3e-471a-a021-5f329e6598b9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LCHs: ['carnivore.n.01']\n",
            "cat.n.01 → feline.n.01 → carnivore.n.01 → canine.n.02 → dog.n.01\n",
            "\n",
            "LCHs: ['carnivore.n.01']\n",
            "dog.n.01 → canine.n.02 → carnivore.n.01 → feline.n.01 → cat.n.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get frame info from lemmas"
      ],
      "metadata": {
        "id": "mFoGKsvjvcBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Token, Doc\n",
        "from spacy import displacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "hr31dbWvvf31"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get a dict of all dependency schemas matching a synset's frames/usage"
      ],
      "metadata": {
        "id": "VtA_LjJE0R-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.tokens import Token\n",
        "\n",
        "# Register a custom extension for original POS if you want to store it\n",
        "if not Token.has_extension(\"orig_tag\"):\n",
        "    Token.set_extension(\"orig_tag\", default=None)\n",
        "\n",
        "@spacy.language.Language.component(\"custom_pos_modifier\")\n",
        "def custom_pos_modifier(doc):\n",
        "    mapping = {\"something\": \"NN\", \"someone\": \"NN\"}  # Use suitable tag string\n",
        "    for token in doc:\n",
        "        # Save original tag\n",
        "        token._.orig_tag = token.tag_\n",
        "\n",
        "        if token.text.lower() not in mapping and token.pos_ == \"NOUN\":\n",
        "            token.tag_ = \"VB\"  # Set fine-grained tag to verb\n",
        "        # Else, you could override mapping if needed\n",
        "\n",
        "    return doc\n",
        "\n",
        "arg_schema_nlp = spacy.load(\"en_core_web_sm\")\n",
        "arg_schema_nlp.add_pipe(\"custom_pos_modifier\", after=\"tagger\")\n",
        "print(arg_schema_nlp.pipe_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc9WPYWq_Dvx",
        "outputId": "c2685a1c-764a-4916-f2c5-b8b4d5a422be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['tok2vec', 'tagger', 'custom_pos_modifier', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: retain original arg structure and add other syntactic/lexical info for better SRL\n",
        "def _flattened_fn_arg_schema(doc: Doc) -> Dict[str, str]:\n",
        "      # used as fallback if FE parsing fails later on\n",
        "      schema_atom_synset_maps = {\n",
        "          \"something\": \"entity.n.01\",\n",
        "          \"someone\": \"causal_agent.n.01\",\n",
        "      }\n",
        "      arg_schema = dict()\n",
        "      for tok in doc:\n",
        "          if tok.lower in schema_atom_synset_maps.keys():\n",
        "              arg_schema[tok.dep_] = schema_atom_synset_maps[tok.lower]\n",
        "          else:\n",
        "              arg_schema[tok.dep_] = tok.lemma_\n",
        "      return arg_schema"
      ],
      "metadata": {
        "id": "1p1LEnCu1arx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_f_id_arg_struct_dict(syn: wn.synset) -> Dict[int, Dict[str, str]]:\n",
        "      syn_frame_ids_strs: Dict[int, Doc] = dict()\n",
        "      for lemma in syn.lemmas():\n",
        "\n",
        "          # get all FrameNet frame IDs for this lemma\n",
        "          for i, f_id in enumerate(lemma.frame_ids()):\n",
        "\n",
        "              # get the argument structure for this frame as a string\n",
        "              f_str = lemma.frame_strings()[i]\n",
        "              # parse f_str into an argument structure vector\n",
        "              #   use tuple to make it hashable\n",
        "              f_arg_structure = f_str.split(' ')\n",
        "\n",
        "              # remove any extra arguments beyond subject, object, theme\n",
        "              if f_arg_structure[-1] != f_arg_structure[-1].lower():\n",
        "                  f_arg_structure = f_arg_structure[:-1]\n",
        "\n",
        "              # create a spacy doc for the argument structure template\n",
        "              doc = arg_schema_nlp(' '.join(f_arg_structure))\n",
        "              # # display dependency parse tree for debugging\n",
        "              # displacy.render(doc, style='dep')\n",
        "\n",
        "              arg_schema_reqs = _flattened_fn_arg_schema(doc)\n",
        "              syn_frame_ids_strs[f_id] = arg_schema_reqs\n",
        "\n",
        "      return syn_frame_ids_strs"
      ],
      "metadata": {
        "id": "4Xtwh7hLy6aM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_id_docs = _get_f_id_arg_struct_dict(wn.synset('spin.v.01'))"
      ],
      "metadata": {
        "id": "VkCbL9cbzh64"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get candidate frames that match the dependency schemes"
      ],
      "metadata": {
        "id": "HCElGXuM0bCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_candidate_wn_frames(pred_tok: spacy.tokens.Token) -> Dict[str, Set[Tuple[bool, bool, bool]]]:\n",
        "      arg_schema_dep_req_map = {\n",
        "          \"subject\": {\"nsubj\", \"nsubjpass\"},\n",
        "          \"object\": {\"dobj\", \"dative\"},\n",
        "          \"theme\": {\"iobj\"}\n",
        "      }\n",
        "      vect_order = [\"subject\", \"object\", \"theme\"]\n",
        "\n",
        "      # get candidate WordNet synsets for the predicate (verbs)\n",
        "      pred_lemma = pred_tok.lemma_.lower()\n",
        "      # get a dict of synset names and synset objects for quick lookup\n",
        "      pred_synsets = wn.synsets(pred_lemma, pos=wn.VERB)\n",
        "      pred_synsets = {syn.name(): syn for syn in pred_synsets}\n",
        "      # create a dict of synset names and lists of their possible frames,\n",
        "      #   indexed by argument structure (num args of each type)\n",
        "      pred_frames: Dict[str, Set[Tuple[bool, bool, bool]]] = dict()\n",
        "\n",
        "      # get all FrameNet frames associated with this synset\n",
        "      for s_name, syn in pred_synsets.items():\n",
        "          for frame_id, arg_schema_reqs in _get_f_id_arg_struct_dict(syn).items():\n",
        "              # restructure arg_reqs based to only evaluate core dependencies\n",
        "              #   TODO: add more relationships for finer-grained SRL\n",
        "              dep_reqs = {\n",
        "                  k: len(v.intersection(set(arg_schema_reqs.keys()))) > 0\n",
        "                  for k, v in arg_schema_dep_req_map.items()\n",
        "              }\n",
        "              # add wordnet's frame requirements as an entry in the synset's nested dict\n",
        "              arg_struct_tuple = tuple(dep_reqs[k] for k in vect_order)\n",
        "              if s_name not in pred_frames:\n",
        "                  pred_frames[s_name] = set()\n",
        "              pred_frames[s_name].add(arg_struct_tuple)\n",
        "\n",
        "      return pred_frames"
      ],
      "metadata": {
        "id": "pU-sJtLzxPpo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "pprint.pprint(_get_candidate_wn_frames(nlp(\"strike\")[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Fh-ElkL5NSk",
        "outputId": "ad974e06-a712-489a-f683-713573412440"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'affect.v.05': {(False, False, False)},\n",
            " 'assume.v.05': {(True, True, False)},\n",
            " 'come_to.v.03': {(False, False, False), (True, True, False)},\n",
            " 'fall.v.08': {(False, False, False)},\n",
            " 'fall_upon.v.01': {(True, True, False)},\n",
            " 'hit.v.02': {(False, False, False)},\n",
            " 'hit.v.05': {(True, False, False), (False, False, False)},\n",
            " 'hit.v.09': {(False, False, False)},\n",
            " 'hit.v.12': {(True, True, False)},\n",
            " 'mint.v.01': {(True, True, False)},\n",
            " 'strickle.v.02': {(True, True, False)},\n",
            " 'strike.v.01': {(False, False, False),\n",
            "                 (True, False, False),\n",
            "                 (True, True, False)},\n",
            " 'strike.v.04': {(True, False, False), (True, True, False)},\n",
            " 'strike.v.05': {(False, False, False)},\n",
            " 'strike.v.07': {(False, False, False)},\n",
            " 'strike.v.10': {(True, True, False)},\n",
            " 'strike.v.11': {(False, False, False)},\n",
            " 'strike.v.13': {(False, False, False), (True, True, False)},\n",
            " 'strike.v.14': {(True, True, False)},\n",
            " 'strike.v.20': {(True, False, False), (False, False, False)},\n",
            " 'strike.v.21': {(True, True, False)}}\n"
          ]
        }
      ]
    }
  ]
}